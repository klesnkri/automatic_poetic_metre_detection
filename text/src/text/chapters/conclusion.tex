\chapter*{Conclusion}\addcontentsline{toc}{chapter}{Conclusion}\markboth{Conclusion}{Conclusion}

The defined objectives of this work are to reimplement the KVĚTA data-driven approach, train the BiLSTM-CRF sequence tagging model with various proposed input configurations, and decide -- based on the obtained results -- whether using the BiLSTM-CRF model for the metrical tagging of Czech syllabotonic verse is successful and has some benefits over using the KVĚTA data-driven approach.

Regarding the KVĚTA approach, it is reimplemented, but with minor changes to the original functionality, so the obtained predictions are probably a bit worse than they would be with the original program.

The BiLSTM-CRF model is successfully trained on many different input configurations, and with the best input configurations, it returns better results than the KVĚTA reimplementation with respect to all evaluated accuracies (syllable-level, line-level, and poem-level). Many observations about the BiLSTM-CRF input are made such that, not surprisingly, the syllable input yields better results than the token input (especially for the poem-level accuracy), but results for the token input, which does not need prior syllabification, are promising (particularly for the line-level accuracy). Another observation is that inputting a lemma significantly improves results for the token input, and the more additional features inputted (author of a poem, year of publication, POS tag, lemma), the better the predictions. The most interesting finding is that the best results are obtained by inputting sequences representing whole poems with line indices on the input that allow a model to distinguish different lines (especially the poem-level accuracy is significantly improved). The approach of inputting sequences representing whole poems may never have been tested before.

Overall, using the BiLSTM-CRF model for metrical tagging of Czech syllabotonic verse represents a great success and has many benefits over using the KVĚTA approach. It does not need to encode any complicated expert knowledge, as everything is learnt automatically by the machine learning model. The machine learning model can even encode its expert knowledge that humans do not understand. Furthermore, it is completely automatic, unlike the KVĚTA approach, which sometimes needs human assistance. On the other hand, with the KVĚTA approach, one has the confidence that for every poem, the returned metrical pattern is justified.

In the future, further experiments could be performed with the BiLSTM-CRF model, such as fine-tuning the model hyperparameters (optimizer with its settings, number of BiLSTM layers, number of recurrent units in one BiLSTM layer \ldots), using character-based representations of the word embeddings,
or training and using different word embeddings (e.g. GloVe, FastText, ELMo \ldots). Syllabification without the need for phonetic transcription could be tried either by using hyphenation approaches or by training own machine learning model. Another interesting experiment could be multitask learning (e.g. predicting the metrical pattern and the POS tag together), predicting the name of the metre (dactyl, trochee \ldots) instead of or alongside the metrical pattern in multitask setup, or transfer learning between poetic corpora in different languages. Furthermore, other machine learning architectures (e.g. transformers) could be tested.
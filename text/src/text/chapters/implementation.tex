\chapter{Implementation}\label{chap:implementation}
\begin{chapterabstract}
This chapter discusses approaches aiming to solve the metrical analysis task. First, the reimplementation of the KVĚTA data-driven approach, later using machine learning and training the BiLSTM-CRF model.
\end{chapterabstract}

\section{Used datasets}\label{section:used-data-sets}
Two different datasets obtained from the Corpus of Czech Verse data are used.

The first dataset contains no polymetric poems, no multimetric verses, no unrecognised metrical positions, and no annotation errors. This dataset contains 57 339 poems.

The second dataset contains no unrecognised metrical positions, no multimetric verses, and no annotations errors as well. However, this time, polymetric poems can be present. This dataset contains 59 661 poems.

Both datasets are divided into training, validation, and testing data using a ratio of 70:15:15. The KVĚTA reimplementation is trained on the training and validation data and tested on the testing data. The BiLSTM-CRF model is trained on the training data, finetuned during training on the validation data, and final results are obtained for the previously unseen testing data.

\section{KVĚTA reimplementation}
\subsection{Syllabification using X-SAMPA transcription}
In the reimplementation of the KVĚTA data-driven approach, a phonetic transcription algorithm is not implemented. Instead, phonetic transcriptions already generated by KVĚTA and published within the Corpus of Czech Verse are used. The X-SAMPA format is parsed to obtain the number of sonority peaks and, therefore, the number of syllables for each word. For the X-SAMPA symbols that are counted, see Tables~\ref{tab:sampa-diphthongs}, \ref{tab:sampa-vocals}, \ref{tab:sampa-long-vocals}, and~\ref{tab:sampa-syllabic-consonant}.

\begin{table}[htpb]
\centering
\caption{X-SAMPA: diphthongs}\label{tab:sampa-diphthongs}
\begin{tabular}{|c||l|l|}\hline
    X-SAMPA Transcription & Example Czech Word & Word Transcription\\\hline\hline
    a\_u & auto (car) & a\_uto / ?a\_uto\\
    E\_u & neuron (neuron) & nE\_uron\\
    o\_u & soud (court) & so\_ut\\\hline
\end{tabular}
\end{table}

\begin{table}[htpb]
\centering
\caption{X-SAMPA: vocals}\label{tab:sampa-vocals}
\begin{tabular}{|c||l|l|}\hline
    X-SAMPA Transcription & Example Czech Word & Word Transcription\\\hline\hline
    a & sad (orchard) & sat\\
    E & pes (dog) & pEs\\
    I & list (leaf) & lIst\\
    o & rok (year) & rok\\
    u & sud (barrel) & sut\\\hline
\end{tabular}
\end{table}

\begin{table}[htpb]
\centering
\caption{X-SAMPA: long vocals}\label{tab:sampa-long-vocals}
\begin{tabular}{|c||l|l|}\hline
    X-SAMPA Transcription & Example Czech Word & Word Transcription\\\hline\hline
    a: & sám (alone) & sa:m\\
    E: & lék (medicine) & lE:k\\
    i: & sýr (cheese) & si:r\\
    o: & tón (tone) & to:n\\
    u: & sůl (salt) & su:l\\\hline
\end{tabular}
\end{table}

\begin{table}[htpb]
\centering
\caption{X-SAMPA: syllabic consonants}\label{tab:sampa-syllabic-consonant}
\begin{tabular}{|c||l|l|}\hline
    X-SAMPA Transcription & Example Czech Word & Word Transcription\\\hline\hline
    = & krk, vlk (throat, wolf) & kr=k, vl=k\\\hline
\end{tabular}
\end{table}

\subsection{Syllable classes}
In the paper~\cite{KVETA} explaining the KVĚTA data-driven approach, the final Syllable class count is reduced to only 12 different Syllable classes. As not all the reductions applied are properly explained in the paper, the reimplementation got to the number of 15 different Syllable classes.

\subsection{Metre generation}
The reimplementation generates the same metres as KVĚTA -- the standard syllabotonic metres; imitations of quantitative syllabic strophes; imitations of hexametre, pentametre, and elegiac couplet; and ghazal poems.

\subsection{Metre assignment}
In the reimplementation, the conditional probability of a Syllable class being realised by a strong or weak position $P(x_{w,i,j}|\sigma_{i,j})$ is not counted using Bayes' Theorem as it is inside KVĚTA (see Equation~\eqref{eq:kveta-data-driven-0}), but is counted directly. In my opinion, counting the probability using Bayes' theorem is, in this case, unnecessary and does not add any benefit compared to counting it directly. Moreover, in my opinion, the probabilities of strong and weak positions should not be considered equal, as the weak position is a bit more likely to occur depending on the structure of all the syllabotonic feet. Probabilities are estimated from training and validation datasets. Yates' $\chi^2$ test is not performed as it adds another complexity to the task, and, therefore, the probabilities are not modified for outlier authors (see Equation~\eqref{eq:kveta-data-driven-1}).

When selecting the metre, the metrical coefficients of the individual line patterns and the overall metrical coefficients are calculated the same as in KVĚTA (see Equations~\eqref{eq:kveta-data-driven-2}, \eqref{eq:kveta-data-driven-3}, and~\eqref{eq:kveta-data-driven-4}). When the same metrical pattern is generated with different metre names, standard syllabotonic metres are selected with priority as in KVĚTA.

\section{BiLSTM-CRF}
Based on the research of machine learning approaches done in Section~\ref{section:ml-approaches}, the BiLSTM-CRF model is selected for training.
 
\subsection{Used implementation}
One of the publicly available implementations of BiLSTM-CRF for sequence tagging~\cite{GitBiLSTMCRF} is used to train the BiLSTM-CRF model. The same implementation is used in the paper~\cite{MetricalTaggingInTheWild}. It uses Keras version 2.2.0 with Tensorflow 1.8.0 as the backend. It must be run using Python 3.6 or lower to be compatible with the Tensorflow version.~\cite{GitBiLSTMCRF}
 
\subsection{Input data format}
When training the BiLSTM-CRF model, two different input data formats are tested:
\begin{itemize}
    \item token-level,
    \item syllable-level.
\end{itemize}

Both formats are inputted using two different approaches:
\begin{itemize}
    \item one input sequence represents a line in a poem,
    \item one input sequence represents a whole poem.
\end{itemize}

For the token-level input data format, the model is inputted a sequence of individual tokens (words) from the poem. For each token, a sequence of metrical positions is predicted. This sequence can be empty as well, when the token is non-accented.

For the syllable-level input data format, the model is inputted a sequence of individual syllables from the poem. For every syllable, exactly one metrical position is predicted. Because the Corpus of Czech Verse lacks annotation of the syllable boundaries, two approaches to obtain syllable-level data are tested: syllabification using hyphenation tools and X-SAMPA syllables.

\subsubsection{Syllabification using hyphenation tools}
Approach to syllabification using phonology-based Czech \TeX{} hyphenation patterns~\cite{UnreasonableEffectiveness} with Frank Liang’s hyphenation algorithm~\cite{HyphenationLiang} is tested. The implementation of Frank Liang’s algorithm published by Ned Batchelder~\cite{HyphenateBatchelder} is modified to support Czech patterns. The problem encountered is that, for example, the word \emph{houska} is hyphenated as \emph{hou-s-ka} due to the ambiguity of the syllable boundaries where both \emph{hous-ka} and \emph{hou-ska} represent valid syllable splits. Therefore, the output has to be further processed to identify and merge these splits.

The next hyphenation tool tested is the Pyphen~\cite{Pyphen} program, which uses Hunspell hyphenation dictionaries and supports the Czech language. It does not annotate both syllable boundaries in case of ambiguity, as Czech \TeX{} hyphenation patterns do.

For the training data of the second dataset used (see Section~\ref{section:used-data-sets}) the average difference from the correct number of syllables in one token is 0.21 for Czech \TeX{} hyphenation patterns, 0.06 for Pyphen and 0.15 for Czech \TeX{} hyphenation patterns with applied heuristics, that one-letter syllables are merged into the previous ones. For Czech \TeX{} hyphenation patterns, on average, 21.63~\% of tokens in one line have an incorrect number of syllables assigned, for Pyphen 6.18~\% and for Czech \TeX{} hyphenation patterns with heuristics 15.31~\%. The referential syllable counts are obtained by splitting X-SAMPA transcriptions using sonority peak symbols (see Tables~\ref{tab:sampa-diphthongs}, \ref{tab:sampa-vocals}, \ref{tab:sampa-long-vocals}, and~\ref{tab:sampa-syllabic-consonant}).

As the differences between the referential syllable counts and the syllable counts obtained by the hyphenation approaches are not so insignificant, it is decided not to use the hyphenation tools at the moment. Furthermore, knowledge about non-syllabic prepositions would have to be incorporated when using these approaches.

\subsubsection{X-SAMPA syllables}
As the hyphenation approaches to syllabification are rejected, X-SAMPA phonetic transcriptions are parsed to obtain syllable representations with correct syllable count. For each word, its X-SAMPA transcription is divided into X-SAMPA syllables using transcriptions of sonority peaks (see Tables~\ref{tab:sampa-diphthongs}, \ref{tab:sampa-vocals}, \ref{tab:sampa-long-vocals}, or and~\ref{tab:sampa-syllabic-consonant}). For non-syllabic words without a sonority peak, no X-SAMPA syllables are generated.

As an example, for the token \emph{blýskajícím} with X-SAMPA transcription \verb|bli:skaji:t_si:m| the following four X-SAMPA syllables are obtained: \verb|[bli:, ska, ji:, t_s:m]|.

\subsection{Input features}
In accordance with the intuition of the paper~\cite{MetricalTaggingInTheWild} no character-based representation of the input is used. The decision is made not to use the capitalisation feature, as the capitalisation of words seems to be of minor importance for this task, and words inside poems usually do not contain numerals.

As word embeddings, Word2Vec embeddings are pre-trained on the training data. By mistake, for the syllable input, Word2Vec embeddings are pre-trained also on the non-syllabic tokens that are not inputted to the model. Due to time reasons, the models are not retrained. However, the assumption is that it should not affect the results significantly.

The implementation supports inputting other features in addition to the input tokens. Following intuition from~\cite{KVETA}, author of a poem, year of publication, POS tag, or lemma is also inputted. An embedding of size 10 is assigned automatically to every input value and concatenated with other input embeddings.

\subsection{Training}
Training loop denoted in Figure~\ref{fig:bi-lstm-crf-trainig} is performed.

The implementation is optimised for speed, it groups sentences with the same lengths together during training, and thanks to that, it is much faster than other BiLSTM-CRF implementations.~\cite{GitBiLSTMCRF} Even though the implementation is highly configurable, the hyperparameters are not further fine-tuned, as training of one input configuration takes 1 to 2 days (CPU is used, training on GPU is not tested).

\subsection{Training hyperparameters}
The hyperparameters are set according to the recommendations in~\cite{MetricalTaggingInTheWild} and~\cite{BiLSTMCRFHyperparameters}.

Following intuition from~\cite{MetricalTaggingInTheWild}, variational dropout is used, which drops 25~\% in the output and recurrent connections, and three BiLSTM layers with 100 recurrent units are trained inside each layer.

In accordance with the findings of~\cite{BiLSTMCRFHyperparameters}, Nadam optimizer is utilised. Thirty-two sentences are used for mini-batch training, as the training set is relatively large. To fight the exploding gradient problem, no gradient clipping, but gradient normalization is performed with threshold value 1. Models are trained for 15 epochs. Training is stopped earlier if the best validation score does not increase for more than five training epochs.

\subsection{Example configuration of hyperparameters}
For a concrete example of the configuration of hyperparameters, see Figure~\ref{fig:bilstm-crf-hyperparameters-config}.

\begin{figure}[htpb]
\centering
\begin{minted}[tabsize=2,breaklines]{js}
{
    "dropout": [0.25, 0.25],
    "classifier": ["CRF"],
    "LSTM-Size": [100, 100, 100],
    "optimizer": "nadam",
    "earlyStopping": 5,
    "miniBatchSize": 32,
    "charEmbeddings": None,
    "clipvalue": 0,
    "clipnorm": 1,
    "featureNames": ["tokens", "lineIdx", "author", "year", "pos", "lemma"],
    "addFeatureDimensions": 10
}
\end{minted}
\caption{BiLSTM-CRF library: Example configuration of hyperparameters}\label{fig:bilstm-crf-hyperparameters-config}
\end{figure}